{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:12.423769Z",
     "start_time": "2020-05-17T15:48:08.608898Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import timeit\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import pytrec_eval\n",
    "\n",
    "from transformers import WEIGHTS_NAME, BertConfig, BertTokenizer, AlbertConfig, AlbertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from retriever_utils import RetrieverDataset, GenPassageRepDataset\n",
    "from modeling import BertForRetrieverOnlyPositivePassage, AlbertForRetrieverOnlyPositivePassage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:12.433946Z",
     "start_time": "2020-05-17T15:48:12.428143Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ALL_MODELS = list(BertConfig.pretrained_config_archive_map.keys())\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForRetrieverOnlyPositivePassage, BertTokenizer),\n",
    "    'albert': (AlbertConfig, AlbertForRetrieverOnlyPositivePassage, AlbertTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:12.671067Z",
     "start_time": "2020-05-17T15:48:12.436991Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:12.874534Z",
     "start_time": "2020-05-17T15:48:12.674860Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter(os.path.join(args.output_dir, 'logs'))\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(\n",
    "        train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, num_workers=args.num_workers)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (\n",
    "            len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(\n",
    "            train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    args.warmup_steps = int(t_total * args.warmup_portion)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(\n",
    "            model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        # model.to(f'cuda:{model.device_ids[0]}')\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank,\n",
    "                                                          find_unused_parameters=True)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\",\n",
    "                args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "                args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 1\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs),\n",
    "                            desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    # Added here for reproductibility (even between python 2 and 3)\n",
    "    set_seed(args)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\",\n",
    "                              disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = {k: v.to(args.device) for k, v in batch.items() if k not in ['example_id', 'qid']}\n",
    "            inputs = {}\n",
    "            if args.given_query:\n",
    "                inputs['query_input_ids'] = batch['query_input_ids']\n",
    "                inputs['query_attention_mask'] = batch['query_attention_mask']        \n",
    "                inputs['query_token_type_ids'] = batch['query_token_type_ids']\n",
    "            if args.given_passage:\n",
    "                inputs['passage_input_ids'] = batch['passage_input_ids']\n",
    "                inputs['passage_attention_mask'] = batch['passage_attention_mask']        \n",
    "                inputs['passage_token_type_ids'] = batch['passage_token_type_ids']\n",
    "                inputs['retrieval_label'] = batch['retrieval_label']\n",
    "            outputs = model(**inputs)\n",
    "            # model outputs are always tuple in transformers (see doc)\n",
    "            if args.given_query and args.given_passage:\n",
    "                loss = outputs[0]\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    if args.local_rank == -1 and args.evaluate_during_training:\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\n",
    "                                'eval_{}'.format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\n",
    "                        'lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\n",
    "                        'loss', (tr_loss - logging_loss)/args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(\n",
    "                        args.output_dir, 'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Take care of distributed/parallel training\n",
    "                    model_to_save = model.module if hasattr(\n",
    "                        model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    torch.save(args, os.path.join(\n",
    "                        output_dir, 'training_args.bin'))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:13.009370Z",
     "start_time": "2020-05-17T15:48:12.877796Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.output_dir)\n",
    "    predict_dir = os.path.join(args.output_dir, 'predictions')\n",
    "    if not os.path.exists(predict_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(predict_dir)\n",
    "\n",
    "    passage_ids, passage_reps = gen_passage_rep(args, model, tokenizer)\n",
    "    passage_reps = np.asarray(passage_reps, dtype='float32')\n",
    "    qids, query_reps = retrieve(args, model, tokenizer)\n",
    "    query_reps = np.asarray(query_reps, dtype='float32')\n",
    "        \n",
    "    index = faiss.IndexFlatIP(args.proj_size)\n",
    "    index.add(passage_reps)\n",
    "    D, I = index.search(query_reps, 5)\n",
    "    \n",
    "    # print(qids, query_reps, passage_ids, passage_reps, D, I)\n",
    "\n",
    "    run = {}\n",
    "    for qid, retrieved_ids, scores in zip(qids, I, D):\n",
    "        run[qid] = {passage_ids[retrieved_id]: float(score) for retrieved_id, score in zip(retrieved_ids, scores)}\n",
    "\n",
    "    with open(args.qrels) as handle:\n",
    "        qrels = json.load(handle)\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "            qrels, {'recip_rank', 'recall'})\n",
    "    metrics = evaluator.evaluate(run)\n",
    "    mrr_list = [v['recip_rank'] for v in metrics.values()]\n",
    "    recall_list = [v['recall_5'] for v in metrics.values()]\n",
    "    eval_metrics = {'MRR': np.average(mrr_list), 'Recall': np.average(recall_list)}\n",
    "\n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:13.164262Z",
     "start_time": "2020-05-17T15:48:13.015045Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_passage_rep(args, model, tokenizer):\n",
    "    # dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "    DatasetClass = GenPassageRepDataset\n",
    "    dataset = DatasetClass(args.gen_passage_rep_input, tokenizer,\n",
    "                           args.load_small, \n",
    "                           passage_max_seq_length=args.passage_max_seq_length)\n",
    "\n",
    "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    # eval_sampler = SequentialSampler(\n",
    "    #     dataset) if args.local_rank == -1 else DistributedSampler(dataset)\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, num_workers=args.num_workers)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        # model.to(f'cuda:{model.device_ids[0]}')\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Gem passage rep *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    run_dict = {}\n",
    "    start_time = timeit.default_timer()\n",
    "    fout = open(args.gen_passage_rep_output, 'w')\n",
    "    passage_ids = []\n",
    "    passage_reps_list = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        example_ids = np.asarray(\n",
    "            batch['example_id']).reshape(-1).tolist()\n",
    "        passage_ids.extend(example_ids)\n",
    "        batch = {k: v.to(args.device)\n",
    "                 for k, v in batch.items() if k != 'example_id'}\n",
    "        with torch.no_grad():\n",
    "            inputs = {}\n",
    "            inputs['passage_input_ids'] = batch['passage_input_ids']\n",
    "            inputs['passage_attention_mask'] = batch['passage_attention_mask']\n",
    "            inputs['passage_token_type_ids'] = batch['passage_token_type_ids']\n",
    "            outputs = model(**inputs)\n",
    "            passage_reps = outputs[0]\n",
    "            passage_reps_list.extend(to_list(passage_reps))\n",
    "        \n",
    "        # with open(args.gen_passage_rep_output, 'w') as fout:\n",
    "        for example_id, passage_rep in zip(example_ids, to_list(passage_reps)):\n",
    "            fout.write(json.dumps({'id': example_id, 'rep': passage_rep}) + '\\n')\n",
    "    fout.close()\n",
    "    return passage_ids, passage_reps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:13.302532Z",
     "start_time": "2020-05-17T15:48:13.168548Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve(args, model, tokenizer, prefix=''):\n",
    "    if prefix == 'test':\n",
    "        eval_file = args.test_file\n",
    "    else:\n",
    "        eval_file = args.dev_file\n",
    "\n",
    "    # dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "    DatasetClass = RetrieverDataset\n",
    "    dataset = DatasetClass(eval_file, tokenizer,\n",
    "                           args.load_small, args.history_num,\n",
    "                           query_max_seq_length=args.query_max_seq_length,\n",
    "                           passage_max_seq_length=args.passage_max_seq_length,\n",
    "                           is_pretraining=args.is_pretraining,\n",
    "                           given_query=True,\n",
    "                           given_passage=False)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    # eval_sampler = SequentialSampler(\n",
    "    #     dataset) if args.local_rank == -1 else DistributedSampler(dataset)\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, num_workers=args.num_workers)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        # model.to(f'cuda:{model.device_ids[0]}')\n",
    "        \n",
    "    # Eval!\n",
    "    logger.info(\"***** Retrieve {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    all_qids = []\n",
    "    all_query_reps = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        qids = np.asarray(\n",
    "            batch['qid']).reshape(-1).tolist()\n",
    "        batch = {k: v.to(args.device)\n",
    "                 for k, v in batch.items() if k not in ['example_id', 'qid']}\n",
    "        with torch.no_grad():\n",
    "            inputs = {}\n",
    "            inputs['query_input_ids'] = batch['query_input_ids']\n",
    "            # print(inputs['query_input_ids'], inputs['query_input_ids'].size())\n",
    "            inputs['query_attention_mask'] = batch['query_attention_mask']\n",
    "            inputs['query_token_type_ids'] = batch['query_token_type_ids']\n",
    "            outputs = model(**inputs)\n",
    "            query_reps = outputs[0]\n",
    "\n",
    "        all_qids.extend(qids)\n",
    "        all_query_reps.extend(to_list(query_reps))\n",
    "\n",
    "    return all_qids, all_query_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:32.168964Z",
     "start_time": "2020-05-17T15:48:13.306482Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/17/2020 11:48:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/17/2020 11:48:15 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-config.json from cache at /mnt/scratch/chenqu/huggingface_cache/albert_v1/8eff152e0e6c9b5bca31d5ed10ab2b543d201465c907d1e7d6a8cedb28ec3a7f.422e3ad6212153abef6df151efafdff1939214d0acdbc8c8011e538a0c2a8e99\n",
      "05/17/2020 11:48:15 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "05/17/2020 11:48:15 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-spiece.model from cache at /mnt/scratch/chenqu/huggingface_cache/albert_v1/e941f532bbbf6d6b7c96efbde9c15d38fc236e7fb120158bfc766814e6170529.c81d4deb77aec08ce575b7a39a989a79dd54f321bfb82c2b54dd35f52f8182cf\n",
      "05/17/2020 11:48:15 - INFO - modeling -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/albert-base-pytorch_model.bin from cache at /mnt/scratch/chenqu/huggingface_cache/albert_v1/b4dc4c13a9a5cd7e3914c30757df86941bdcfb71239e068a99708292173f73d5.b8abbff368ccef39a460388d8ab168a953203681721c79c7e10826db7cb7468d\n",
      "05/17/2020 11:48:16 - INFO - modeling -   Weights of AlbertForRetrieverOnlyPositivePassage not initialized from pretrained model: ['query_proj.weight', 'query_proj.bias', 'passage_proj.weight', 'passage_proj.bias']\n",
      "05/17/2020 11:48:19 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='/mnt/scratch/chenqu/huggingface_cache/albert_v1/', config_name='', dev_file='/mnt/scratch/chenqu/orconvqa/v5/quac_canard/preprocessed/dev.txt', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_test=False, do_train=True, eval_all_checkpoints=True, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gen_passage_rep=False, gen_passage_rep_input='/mnt/scratch/chenqu/orconvqa/v5/test_retriever/dev_blocks.txt', gen_passage_rep_output='/mnt/scratch/chenqu/orconvqa_output/retriever_release_test/dev_blocks.txt', given_passage=True, given_query=True, gradient_accumulation_steps=1, history_num=1, is_pretraining=True, learning_rate=5e-05, load_small=True, local_rank=-1, logging_steps=1, max_grad_norm=1.0, max_steps=-1, model_name_or_path='albert-base-v1', model_type='albert', n_gpu=1, no_cuda=False, num_train_epochs=2.0, num_workers=4, only_positive_passage=True, output_dir='/mnt/scratch/chenqu/orconvqa_output/retriever_release_test', overwrite_cache=False, overwrite_output_dir=True, passage_max_seq_length=384, per_gpu_eval_batch_size=300, per_gpu_train_batch_size=12, proj_size=128, qrels='/mnt/scratch/chenqu/orconvqa/v5/retrieval/qrels.txt', query_max_seq_length=128, retrieve=False, retrieve_checkpoint='/mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-5917', save_steps=20, seed=42, server_ip='', server_port='', test_file='/mnt/scratch/chenqu/orconvqa/v5/quac_canard/preprocessed/test.txt', tokenizer_name='albert-base-v1', train_file='/mnt/scratch/chenqu/orconvqa/v5/quac_canard/preprocessed/train.txt', verbose_logging=False, warmup_portion=0.1, warmup_steps=0, weight_decay=0.0)\n",
      "05/17/2020 11:48:19 - INFO - __main__ -   ***** Running training *****\n",
      "05/17/2020 11:48:19 - INFO - __main__ -     Num examples = 50\n",
      "05/17/2020 11:48:19 - INFO - __main__ -     Num Epochs = 2\n",
      "05/17/2020 11:48:19 - INFO - __main__ -     Instantaneous batch size per GPU = 12\n",
      "05/17/2020 11:48:19 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "05/17/2020 11:48:19 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "05/17/2020 11:48:19 - INFO - __main__ -     Total optimization steps = 10\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  20%|██        | 1/5 [00:02<00:09,  2.35s/it]\u001b[A\n",
      "Iteration:  40%|████      | 2/5 [00:03<00:05,  1.94s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 3/5 [00:04<00:03,  1.65s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 4/5 [00:05<00:01,  1.46s/it]\u001b[A\n",
      "Epoch:  50%|█████     | 1/2 [00:05<00:05,  5.70s/it]/it]\u001b[A\n",
      "Iteration:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  20%|██        | 1/5 [00:02<00:08,  2.18s/it]\u001b[A\n",
      "Iteration:  40%|████      | 2/5 [00:03<00:05,  1.83s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 3/5 [00:04<00:03,  1.58s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 4/5 [00:05<00:01,  1.40s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]/it]\u001b[A\n",
      "05/17/2020 11:48:30 - INFO - __main__ -    global_step = 11, average loss = 2.081991759213534\n",
      "05/17/2020 11:48:30 - INFO - __main__ -   Saving model checkpoint to /mnt/scratch/chenqu/orconvqa_output/retriever_release_test\n",
      "05/17/2020 11:48:30 - INFO - transformers.configuration_utils -   Configuration saved in /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-11/config.json\n",
      "05/17/2020 11:48:30 - INFO - transformers.modeling_utils -   Model weights saved in /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-11/pytorch_model.bin\n",
      "05/17/2020 11:48:31 - INFO - transformers.configuration_utils -   loading configuration file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-11/config.json\n",
      "05/17/2020 11:48:31 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"proj_size\": 128,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "05/17/2020 11:48:31 - INFO - modeling -   loading weights file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-11/pytorch_model.bin\n",
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   Model name '/mnt/scratch/chenqu/orconvqa_output/retriever_release_test' not found in model shortcut name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). Assuming '/mnt/scratch/chenqu/orconvqa_output/retriever_release_test' is a path or url to a directory containing tokenizer files.\n",
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/spiece.model\n",
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/added_tokens.json\n",
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Required parameters\n",
    "parser.add_argument(\"--train_file\", default='/mnt/scratch/chenqu/orconvqa/v5/quac_canard/preprocessed/train.txt',\n",
    "                    type=str, required=False,\n",
    "                    help=\"open retrieval quac json for training. \")\n",
    "# parser.add_argument(\"--train_file\", default='/mnt/scratch/chenqu/orconvqa/v5/test_retriever/google_nq+quac.txt',\n",
    "#                     type=str, required=False,\n",
    "#                     help=\"open retrieval quac json for training. \")\n",
    "parser.add_argument(\"--dev_file\", default='/mnt/scratch/chenqu/orconvqa/v5/quac_canard/preprocessed/dev.txt',\n",
    "                    type=str, required=False,\n",
    "                    help=\"open retrieval quac json for predictions.\")\n",
    "parser.add_argument(\"--test_file\", default='/mnt/scratch/chenqu/orconvqa/v5/quac_canard/preprocessed/test.txt',\n",
    "                    type=str, required=False,\n",
    "                    help=\"open retrieval quac json for predictions.\")\n",
    "parser.add_argument(\"--model_type\", default='albert', type=str, required=False,\n",
    "                    help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "# parser.add_argument(\"--model_name_or_path\", default='bert-base-uncased', type=str, required=False,\n",
    "#                     help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS))\n",
    "parser.add_argument(\"--model_name_or_path\", default='albert-base-v1', type=str, required=False,\n",
    "                    help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS))\n",
    "parser.add_argument(\"--output_dir\", default='/mnt/scratch/chenqu/orconvqa_output/retriever_release_test', type=str, required=False,\n",
    "                    help=\"The output directory where the model checkpoints and predictions will be written.\")\n",
    "parser.add_argument(\"--qrels\", default='/mnt/scratch/chenqu/orconvqa/v5/retrieval/qrels.txt', type=str, required=False,\n",
    "                    help=\"qrels to evaluate open retrieval\")\n",
    "\n",
    "parser.add_argument(\"--given_query\", default=True, type=str2bool,\n",
    "                    help=\"Whether query is given.\")\n",
    "parser.add_argument(\"--given_passage\", default=True, type=str2bool,\n",
    "                    help=\"Whether passage is given.\")\n",
    "parser.add_argument(\"--is_pretraining\", default=True, type=str2bool,\n",
    "                    help=\"Whether is pretraining.\")\n",
    "parser.add_argument(\"--only_positive_passage\", default=True, type=str2bool,\n",
    "                    help=\"we only pass the positive passages, the rest of the passges in the batch are considered as negatives\")\n",
    "parser.add_argument(\"--gen_passage_rep\", default=False, type=str2bool,\n",
    "                    help=\"generate passage representations for all \")\n",
    "parser.add_argument(\"--retrieve_checkpoint\", \n",
    "                    default='/mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-5917', type=str,\n",
    "                    help=\"generate query/passage representations with this checkpoint\")\n",
    "parser.add_argument(\"--gen_passage_rep_input\", \n",
    "                    default='/mnt/scratch/chenqu/orconvqa/v5/test_retriever/dev_blocks.txt', type=str,\n",
    "                    help=\"generate passage representations for this file that contains passages\")\n",
    "parser.add_argument(\"--gen_passage_rep_output\", \n",
    "                    default='/mnt/scratch/chenqu/orconvqa_output/retriever_release_test/dev_blocks.txt', type=str,\n",
    "                    help=\"passage representations\")\n",
    "parser.add_argument(\"--retrieve\", default=False, type=str2bool,\n",
    "                    help=\"generate query reps and retrieve passages\")\n",
    "\n",
    "# Other parameters\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "# parser.add_argument(\"--tokenizer_name\", default=\"bert-base-uncased\", type=str,\n",
    "#                     help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--tokenizer_name\", default=\"albert-base-v1\", type=str,\n",
    "                    help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--cache_dir\", default=\"/mnt/scratch/chenqu/huggingface_cache/albert_v1/\", type=str,\n",
    "                    help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
    "\n",
    "parser.add_argument(\"--query_max_seq_length\", default=128, type=int,\n",
    "                    help=\"The maximum input sequence length of query (125 + [CLS] + [SEP]).\"\n",
    "                         \"125 is the max question length in the reader.\")\n",
    "parser.add_argument(\"--passage_max_seq_length\", default=384, type=int,\n",
    "                    help=\"The maximum input sequence length of passage (384 + [CLS] + [SEP]).\")\n",
    "parser.add_argument(\"--proj_size\", default=128, type=int,\n",
    "                    help=\"The size of the query/passage rep after projection of [CLS] rep.\")\n",
    "parser.add_argument(\"--do_train\", default=True, type=str2bool,\n",
    "                    help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\", default=True, type=str2bool,\n",
    "                    help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_test\", default=False, type=str2bool,\n",
    "                    help=\"Whether to run eval on the test set.\")\n",
    "parser.add_argument(\"--evaluate_during_training\", default=False, type=str2bool,\n",
    "                    help=\"Rul evaluation during training at each logging step.\")\n",
    "parser.add_argument(\"--do_lower_case\", default=True, type=str2bool,\n",
    "                    help=\"Set this flag if you are using an uncased model.\")\n",
    "\n",
    "parser.add_argument(\"--per_gpu_train_batch_size\", default=12, type=int,\n",
    "                    help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--per_gpu_eval_batch_size\", default=300, type=int,\n",
    "                    help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                    help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                    help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                    help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=2.0, type=float,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
    "                    help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
    "                    help=\"Linear warmup over warmup_steps.\")\n",
    "parser.add_argument(\"--warmup_portion\", default=0.1, type=float,\n",
    "                    help=\"Linear warmup over warmup_steps (=t_total * warmup_portion). override warmup_steps \")\n",
    "parser.add_argument(\"--verbose_logging\", action='store_true',\n",
    "                    help=\"If true, all of the warnings related to data processing will be printed. \"\n",
    "                         \"A number of warnings are expected for a normal SQuAD evaluation.\")\n",
    "\n",
    "parser.add_argument('--logging_steps', type=int, default=1,\n",
    "                    help=\"Log every X updates steps.\")\n",
    "parser.add_argument('--save_steps', type=int, default=20,\n",
    "                    help=\"Save checkpoint every X updates steps.\")\n",
    "parser.add_argument(\"--eval_all_checkpoints\", default=True, type=str2bool,\n",
    "                    help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\")\n",
    "parser.add_argument(\"--no_cuda\", default=False, type=str2bool,\n",
    "                    help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument('--overwrite_output_dir', default=True, type=str2bool,\n",
    "                    help=\"Overwrite the content of the output directory\")\n",
    "parser.add_argument('--overwrite_cache', action='store_true',\n",
    "                    help=\"Overwrite the cached training and evaluation sets\")\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help=\"random seed for initialization\")\n",
    "\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                    help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--fp16', default=False, type=str2bool,\n",
    "                    help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
    "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                         \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "parser.add_argument('--server_ip', type=str, default='',\n",
    "                    help=\"Can be used for distant debugging.\")\n",
    "parser.add_argument('--server_port', type=str, default='',\n",
    "                    help=\"Can be used for distant debugging.\")\n",
    "\n",
    "parser.add_argument(\"--load_small\", default=True, type=str2bool, required=False,\n",
    "                    help=\"whether to load just a small portion of data during development\")\n",
    "parser.add_argument(\"--history_num\", default=1, type=int, required=False,\n",
    "                    help=\"number of history turns to use\")\n",
    "parser.add_argument(\"--num_workers\", default=4, type=int, required=False,\n",
    "                    help=\"number of workers for dataloader\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
    "\n",
    "# Setup distant debugging if needed\n",
    "if args.server_ip and args.server_port:\n",
    "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "    import ptvsd\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(\n",
    "        address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.set_device(0)\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "               args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "args.model_type = args.model_type.lower()\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
    "                                      cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "config.proj_size = args.proj_size\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "                                            do_lower_case=args.do_lower_case,\n",
    "                                            cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "model = model_class.from_pretrained(args.model_name_or_path,\n",
    "                                    from_tf=bool(\n",
    "                                        '.ckpt' in args.model_name_or_path),\n",
    "                                    config=config,\n",
    "                                    cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "# model = model_class.from_pretrained(args.retrieve_checkpoint,\n",
    "#                                     from_tf=bool(\n",
    "#                                         '.ckpt' in args.model_name_or_path),\n",
    "#                                     config=config,\n",
    "#                                     cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.\n",
    "# Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n",
    "# remove the need for this code, but it is still valid.\n",
    "if args.fp16:\n",
    "    try:\n",
    "        import apex\n",
    "        apex.amp.register_half_function(torch, 'einsum')\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "\n",
    "if args.retrieve:\n",
    "    args.gen_passage_rep = False\n",
    "    args.do_train = False\n",
    "    args.do_eval = False\n",
    "    args.do_test = False\n",
    "    args.given_query = True\n",
    "    args.given_passage = False\n",
    "\n",
    "if args.gen_passage_rep:\n",
    "    args.do_train = False\n",
    "    args.do_eval = False\n",
    "    args.do_test = False\n",
    "\n",
    "# if args.do_eval or args.evaluate_during_training or args.retrieve:\n",
    "#     eval_qrels = {}\n",
    "#     with open(args.dev_file) as fin:\n",
    "#         for line in fin:\n",
    "#             instance = json.loads(line.strip())\n",
    "#             qid = instance['qid']\n",
    "#             evidences, retrieval_labels = instance['evidences'], instance['retrieval_labels']\n",
    "#             eval_qrels[qid] = {}\n",
    "#             for i, (evidence, retrieval_label) in enumerate(zip(evidences, retrieval_labels)):\n",
    "#                 doc_id = '{}_{}'.format(qid, i)\n",
    "#                 eval_qrels[qid][doc_id] = retrieval_label\n",
    "\n",
    "#     evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "#         eval_qrels, {'recip_rank', 'recall'})\n",
    "\n",
    "# if args.do_test or args.retrieve:\n",
    "#     test_qrels = {}\n",
    "#     with open(args.test_file) as fin:\n",
    "#         for line in fin:\n",
    "#             instance = json.loads(line.strip())\n",
    "#             qid = instance['qid']\n",
    "#             evidences, retrieval_labels = instance['evidences'], instance['retrieval_labels']\n",
    "#             test_qrels[qid] = {}\n",
    "#             for i, (evidence, retrieval_label) in enumerate(zip(evidences, retrieval_labels)):\n",
    "#                 doc_id = '{}_{}'.format(qid, i)\n",
    "#                 test_qrels[qid][doc_id] = retrieval_label\n",
    "\n",
    "#     test_evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "#         test_qrels, {'recip_rank', 'recall'})\n",
    "\n",
    "# Training\n",
    "if args.do_train:\n",
    "    DatasetClass = RetrieverDataset\n",
    "    train_dataset = DatasetClass(args.train_file, tokenizer,\n",
    "                                 args.load_small, args.history_num,\n",
    "                                 query_max_seq_length=args.query_max_seq_length,\n",
    "                                 passage_max_seq_length=args.passage_max_seq_length,\n",
    "                                 is_pretraining=True,\n",
    "                                 given_query=True,\n",
    "                                 given_passage=True, \n",
    "                                 only_positive_passage=args.only_positive_passage)\n",
    "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "    logger.info(\" global_step = %s, average loss = %s\",\n",
    "                global_step, tr_loss)\n",
    "\n",
    "# Save the trained model and the tokenizer\n",
    "if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "    # They can then be reloaded using `from_pretrained()`\n",
    "    # Take care of distributed/parallel training\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    final_checkpoint_output_dir = os.path.join(\n",
    "        args.output_dir, 'checkpoint-{}'.format(global_step))\n",
    "    if not os.path.exists(final_checkpoint_output_dir):\n",
    "        os.makedirs(final_checkpoint_output_dir)\n",
    "\n",
    "    model_to_save.save_pretrained(final_checkpoint_output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(\n",
    "        final_checkpoint_output_dir, 'training_args.bin'))\n",
    "\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = model_class.from_pretrained(\n",
    "        final_checkpoint_output_dir, force_download=True)\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:43.245142Z",
     "start_time": "2020-05-17T15:48:32.171630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   Model name '/mnt/scratch/chenqu/orconvqa_output/retriever_release_test' not found in model shortcut name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). Assuming '/mnt/scratch/chenqu/orconvqa_output/retriever_release_test' is a path or url to a directory containing tokenizer files.\n",
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/spiece.model\n",
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/added_tokens.json\n",
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/special_tokens_map.json\n",
      "05/17/2020 11:48:32 - INFO - transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/tokenizer_config.json\n",
      "05/17/2020 11:48:32 - INFO - __main__ -   Evaluate the following checkpoints: ['/mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-11', '/mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-20']\n",
      "05/17/2020 11:48:32 - INFO - transformers.configuration_utils -   loading configuration file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-11/config.json\n",
      "05/17/2020 11:48:32 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"proj_size\": 128,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "05/17/2020 11:48:32 - INFO - modeling -   loading weights file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-11/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 global_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/17/2020 11:48:33 - INFO - __main__ -   ***** Gem passage rep *****\n",
      "05/17/2020 11:48:33 - INFO - __main__ -     Num examples = 100\n",
      "05/17/2020 11:48:33 - INFO - __main__ -     Batch size = 300\n",
      "Evaluating: 100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n",
      "05/17/2020 11:48:37 - INFO - __main__ -   ***** Retrieve  *****\n",
      "05/17/2020 11:48:37 - INFO - __main__ -     Num examples = 50\n",
      "05/17/2020 11:48:37 - INFO - __main__ -     Batch size = 300\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "05/17/2020 11:48:37 - INFO - transformers.configuration_utils -   loading configuration file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-20/config.json\n",
      "05/17/2020 11:48:37 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"proj_size\": 128,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "05/17/2020 11:48:37 - INFO - modeling -   loading weights file /mnt/scratch/chenqu/orconvqa_output/retriever_release_test/checkpoint-20/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 global_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/17/2020 11:48:39 - INFO - __main__ -   ***** Gem passage rep *****\n",
      "05/17/2020 11:48:39 - INFO - __main__ -     Num examples = 100\n",
      "05/17/2020 11:48:39 - INFO - __main__ -     Batch size = 300\n",
      "Evaluating: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it]\n",
      "05/17/2020 11:48:42 - INFO - __main__ -   ***** Retrieve  *****\n",
      "05/17/2020 11:48:42 - INFO - __main__ -     Num examples = 50\n",
      "05/17/2020 11:48:42 - INFO - __main__ -     Batch size = 300\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "05/17/2020 11:48:43 - INFO - __main__ -   Results: {'MRR_11': 0.0, 'Recall_11': 0.0, 'MRR_20': 0.0, 'Recall_20': 0.0}\n",
      "05/17/2020 11:48:43 - INFO - __main__ -   best metrics: {}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n",
    "\n",
    "results = {}\n",
    "max_mrr = 0.0\n",
    "best_metrics = {}\n",
    "if args.do_eval and args.local_rank in [-1, 0]:\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    tb_writer = SummaryWriter(os.path.join(args.output_dir, 'logs'))\n",
    "    checkpoints = [args.output_dir]\n",
    "    if args.eval_all_checkpoints:\n",
    "        checkpoints = list(os.path.dirname(c) for c in sorted(\n",
    "            glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "#         logging.getLogger(\"transformers.modeling_utils\").setLevel(\n",
    "#             logging.WARN)  # Reduce model loading logs\n",
    "\n",
    "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "\n",
    "    for checkpoint in checkpoints:\n",
    "        # Reload the model\n",
    "        global_step = checkpoint.split(\n",
    "            '-')[-1] if len(checkpoint) > 1 else \"\"\n",
    "        print(global_step, 'global_step')\n",
    "        model = model_class.from_pretrained(\n",
    "            checkpoint, force_download=True)\n",
    "        model.to(args.device)\n",
    "\n",
    "        # Evaluate\n",
    "        result = evaluate(args, model, tokenizer, prefix=global_step)\n",
    "        if result['MRR'] > max_mrr:\n",
    "            max_mrr = result['MRR']\n",
    "            best_metrics['MRR'] = result['MRR']\n",
    "            best_metrics['Recall'] = result['Recall']\n",
    "            best_metrics['global_step'] = global_step\n",
    "\n",
    "        for key, value in result.items():\n",
    "            tb_writer.add_scalar(\n",
    "                'eval_{}'.format(key), value, global_step)\n",
    "\n",
    "        result = dict((k + ('_{}'.format(global_step) if global_step else ''), v)\n",
    "                      for k, v in result.items())\n",
    "        results.update(result)\n",
    "\n",
    "    best_metrics_file = os.path.join(\n",
    "        args.output_dir, 'predictions', 'best_metrics.json')\n",
    "    with open(best_metrics_file, 'w') as fout:\n",
    "        json.dump(best_metrics, fout)\n",
    "        \n",
    "    all_results_file = os.path.join(\n",
    "        args.output_dir, 'predictions', 'all_results.json')\n",
    "    with open(all_results_file, 'w') as fout:\n",
    "        json.dump(results, fout)\n",
    "\n",
    "    logger.info(\"Results: {}\".format(results))\n",
    "    logger.info(\"best metrics: {}\".format(best_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:43.255883Z",
     "start_time": "2020-05-17T15:48:43.248442Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.do_test and args.local_rank in [-1, 0]:\n",
    "    best_global_step = best_metrics['global_step']\n",
    "    best_checkpoint = os.path.join(\n",
    "        args.output_dir, 'checkpoint-{}'.format(best_global_step))\n",
    "    logger.info(\"Test the best checkpoint: %s\", best_checkpoint)\n",
    "\n",
    "    model = model_class.from_pretrained(\n",
    "        best_checkpoint, force_download=True)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Evaluate\n",
    "    result = evaluate(args, model, tokenizer, prefix='test')\n",
    "\n",
    "    test_metrics_file=os.path.join(\n",
    "        args.output_dir, 'predictions', 'test_metrics.json')\n",
    "    with open(test_metrics_file, 'w') as fout:\n",
    "        json.dump(result, fout)\n",
    "\n",
    "    logger.info(\"Test Result: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:43.568798Z",
     "start_time": "2020-05-17T15:48:43.258245Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.gen_passage_rep and args.local_rank in [-1, 0]:\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    logger.info(\"Gen passage rep with: %s\", args.retrieve_checkpoint)\n",
    "\n",
    "    model = model_class.from_pretrained(\n",
    "        args.retrieve_checkpoint, force_download=True)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Evaluate\n",
    "    gen_passage_rep(args, model, tokenizer)\n",
    "    \n",
    "    logger.info(\"Gen passage rep complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:43.773686Z",
     "start_time": "2020-05-17T15:48:43.572993Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.retrieve and args.local_rank in [-1, 0]:\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    logger.info(\"Retrieve with: %s\", args.retrieve_checkpoint)\n",
    "    model = model_class.from_pretrained(\n",
    "        args.retrieve_checkpoint, force_download=True)\n",
    "    model.to(args.device)\n",
    "\n",
    "    # Evaluate\n",
    "    qids, query_reps = retrieve(args, model, tokenizer)\n",
    "    query_reps = np.asarray(query_reps, dtype='float32')\n",
    "       \n",
    "    logger.info(\"Gen query rep complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T15:48:43.973649Z",
     "start_time": "2020-05-17T15:48:43.777058Z"
    }
   },
   "outputs": [],
   "source": [
    "# passage_ids, passage_reps = [], []\n",
    "# with open(args.gen_passage_rep_output) as fin:\n",
    "#     for line in tqdm(fin):\n",
    "#         dic = json.loads(line.strip())\n",
    "#         passage_ids.append(dic['id'])\n",
    "#         passage_reps.append(dic['rep'])\n",
    "# passage_reps = np.asarray(passage_reps, dtype='float32')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
